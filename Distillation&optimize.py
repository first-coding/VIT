import torch
import torch.nn as nn
import torch.nn.functional as F
import torch.optim as optim
from torchvision import datasets, transforms
from torch.utils.data import DataLoader
from tqdm import tqdm
# ====== æ•™å¸ˆæ¨¡å‹å¯¼å…¥ ======
from models.teacher_VIT import VisionTransformer  # ç¡®ä¿è·¯å¾„æ­£ç¡®
from models.Student import StudentCNN  
from script.quantize_export_infer import quantize
# ====== è’¸é¦æŸå¤±å‡½æ•°ï¼ˆè¾“å‡ºå±‚ï¼‰======
def kd_loss(student_logits, teacher_logits, T=6.0, alpha=0.6):
    # è®¡ç®—äº¤å‰ç†µæŸå¤±
    ce_loss = F.cross_entropy(student_logits, teacher_logits.argmax(dim=1))
    
    # è®¡ç®—KLæ•£åº¦æŸå¤±
    kl_loss = F.kl_div(
        F.log_softmax(student_logits / T, dim=1),
        F.softmax(teacher_logits / T, dim=1),
        reduction='batchmean'
    ) * (T * T)
    
    return alpha * ce_loss + (1 - alpha) * kl_loss

# ====== æ•°æ®åŠ è½½ ======
def get_dataloader(batch_size=64):
    transform = transforms.Compose([
        transforms.Resize((32, 32)),
        transforms.RandomHorizontalFlip(),  # æ•°æ®å¢å¼º
        transforms.ToTensor()
    ])
    train_set = datasets.CIFAR10(root='./data', train=True, download=False, transform=transform)
    test_set = datasets.CIFAR10(root='./data', train=False, download=False, transform=transform)
    train_loader = DataLoader(train_set, batch_size=batch_size, shuffle=True)
    test_loader = DataLoader(test_set, batch_size=batch_size, shuffle=False)
    return train_loader, test_loader

# ====== æµ‹è¯•å‡½æ•° ======
def evaluate(model, dataloader, device):
    model.eval()
    correct, total = 0, 0
    with torch.no_grad():
        for x, y in dataloader:
            x, y = x.to(device), y.to(device)
            out = model(x)
            pred = out.argmax(dim=1)
            correct += (pred == y).sum().item()
            total += y.size(0)
    acc = correct / total
    return acc

# ====== ä¸»å‡½æ•° ======
def main():
    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')

    train_loader, test_loader = get_dataloader(batch_size=64)

    teacher = VisionTransformer()
    teacher.load_state_dict(torch.load('./Output/teacher/vit_model_cycle_10.pth', map_location=device))
    teacher.to(device)
    teacher.eval()

    student = StudentCNN(num_classes=10).to(device)
    optimizer = optim.Adam(student.parameters(), lr=3e-4)
    scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=5, gamma=0.5)

    epochs = 20
    T = 6.0
    alpha = 0.6
    best_acc = 0.0  # ğŸŸ¡ åˆå§‹åŒ–æœ€ä¼˜ç²¾åº¦

    for epoch in range(epochs):
        student.train()
        total_loss = 0
        loop = tqdm(train_loader, desc=f"Epoch {epoch+1}/{epochs}")
        for imgs, _ in loop:
            imgs = imgs.to(device)
            with torch.no_grad():
                teacher_logits = teacher(imgs)
            student_logits = student(imgs)
            loss = kd_loss(student_logits, teacher_logits, T=T, alpha=alpha)

            optimizer.zero_grad()
            loss.backward()
            optimizer.step()

            total_loss += loss.item()
            loop.set_postfix(loss=loss.item())

        scheduler.step()
        acc = evaluate(student, test_loader, device)
        print(f"âœ… Epoch {epoch+1} | Avg Loss: {total_loss/len(train_loader):.4f} | ğŸ¯ Test Acc: {acc*100:.2f}%")

        # ğŸŸ¢ è‹¥å½“å‰ç²¾åº¦ä¼˜äºæœ€ä½³ï¼Œåˆ™ä¿å­˜æ¨¡å‹
        if acc > best_acc:
            best_acc = acc
            torch.save(student.state_dict(), "./Output/student/best_student_cnn.pth")
            print(f"ğŸ“¦ ä¿å­˜æ–°æœ€ä¼˜æ¨¡å‹ï¼ˆç²¾åº¦: {best_acc*100:.2f}%ï¼‰")

    print("ğŸ è®­ç»ƒå®Œæˆã€‚æœ€ä¼˜æ¨¡å‹å·²ä¿å­˜ä¸º best_student_cnn.pth")
    print("ğŸš€ å¼€å§‹æ‰§è¡Œé™æ€é‡åŒ– + ONNX å¯¼å‡º + æ¨ç†...")
    quantize()


if __name__ == '__main__':
    main()
